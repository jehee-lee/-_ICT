{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytroch_zero_to_all 5강 => 텐서플로우",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNlBq0b8MI9e17Jz4t7ZfB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jehee-lee/hanium/blob/main/pytroch_zero_to_all_5%EA%B0%95_%3D%3E_%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rToYVrGxvpYu",
        "outputId": "d92e2aa1-1322-482a-a706-0b67b7753595"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "x_data = tf.constant([[1.0], [2.0], [3.0]])\n",
        "y_data = tf.constant([[2.0], [4.0], [6.0]])\n",
        "\n",
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.linear = Dense(1, input_dim=1) #one-in / one-out\n",
        "\n",
        "  def call(self, x):\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred\n",
        "\n",
        "model = MyModel()\n",
        "\n",
        "#loss fuction과 optimizer를 세웁니다. \n",
        "#이 때 loss function은 Mean Square error, optimizer는 SGD (learning rate = 0.01로합니다.)\n",
        "loss_object = tf.keras.losses.MSE\n",
        "optimizer = tf.keras.optimizers.SGD(0.01)\n",
        "\n",
        "for epoch in range(500):\n",
        "    with tf.GradientTape() as tape:\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "      y_pred = model(x_data)\n",
        "      # 2) Compute and print loss\n",
        "      loss = loss_object(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss} ')#pytorch와 비교했을 때 초기 가중치 설정이 다르기 때문에 loss 값이 다르게 나옵니다.\n",
        "    \n",
        "    #텐서플로우는 zero_grad를 따로 수행하지 않습니다.\n",
        "    #loss.backoward, 역전파\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    #optimizer.step(), 가중치 갱신\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: [ 7.4712105 29.884842  67.24089  ] \n",
            "Epoch: 1 | Loss: [ 2.6896355 13.017837  31.092188 ] \n",
            "Epoch: 2 | Loss: [ 0.831463   5.6082554 14.62684  ] \n",
            "Epoch: 3 | Loss: [0.1826034 2.3753486 7.0496187] \n",
            "Epoch: 4 | Loss: [0.01109787 0.9795855  3.5123768 ] \n",
            "Epoch: 5 | Loss: [0.01170344 0.38696674 1.8287584 ] \n",
            "Epoch: 6 | Loss: [0.06218259 0.14214903 1.0068476 ] \n",
            "Epoch: 7 | Loss: [0.11715896 0.04571348 0.59274507] \n",
            "Epoch: 8 | Loss: [0.16242024 0.01106234 0.3762215 ] \n",
            "Epoch: 9 | Loss: [0.19560595 0.00108695 0.25827906] \n",
            "Epoch: 10 | Loss: [0.21829237 0.00022324 0.19126233] \n",
            "Epoch: 11 | Loss: [0.2329216  0.00217537 0.15158372] \n",
            "Epoch: 12 | Loss: [0.24173272 0.00455994 0.12716988] \n",
            "Epoch: 13 | Loss: [0.24648622 0.00659338 0.11160545] \n",
            "Epoch: 14 | Loss: [0.24847122 0.0081111  0.10134359] \n",
            "Epoch: 15 | Loss: [0.24859579 0.00916146 0.09434896] \n",
            "Epoch: 16 | Loss: [0.2474865  0.00984684 0.08941171] \n",
            "Epoch: 17 | Loss: [0.24556854 0.01026609 0.08579326] \n",
            "Epoch: 18 | Loss: [0.24312755 0.01049881 0.08303197] \n",
            "Epoch: 19 | Loss: [0.24035592 0.01060363 0.08083427] \n",
            "Epoch: 20 | Loss: [0.2373806  0.0106223  0.07901067] \n",
            "Epoch: 21 | Loss: [0.23428667 0.0105838  0.07743831] \n",
            "Epoch: 22 | Loss: [0.23113027 0.01050761 0.07603604] \n",
            "Epoch: 23 | Loss: [0.22794794 0.01040706 0.07475268] \n",
            "Epoch: 24 | Loss: [0.22476436 0.01029093 0.07355186] \n",
            "Epoch: 25 | Loss: [0.22159533 0.01016526 0.07241132] \n",
            "Epoch: 26 | Loss: [0.21845125 0.01003386 0.07131508] \n",
            "Epoch: 27 | Loss: [0.21533869 0.00989943 0.07025374] \n",
            "Epoch: 28 | Loss: [0.21226166 0.00976364 0.06921943] \n",
            "Epoch: 29 | Loss: [0.20922291 0.00962758 0.06820898] \n",
            "Epoch: 30 | Loss: [0.20622382 0.00949209 0.06721782] \n",
            "Epoch: 31 | Loss: [0.2032653  0.00935757 0.06624471] \n",
            "Epoch: 32 | Loss: [0.20034736 0.00922428 0.06528796] \n",
            "Epoch: 33 | Loss: [0.19747044 0.00909258 0.06434664] \n",
            "Epoch: 34 | Loss: [0.1946339  0.00896246 0.06341984] \n",
            "Epoch: 35 | Loss: [0.1918377  0.008834   0.06250691] \n",
            "Epoch: 36 | Loss: [0.18908124 0.00870727 0.06160795] \n",
            "Epoch: 37 | Loss: [0.18636426 0.00858224 0.06072183] \n",
            "Epoch: 38 | Loss: [0.18368638 0.00845909 0.0598489 ] \n",
            "Epoch: 39 | Loss: [0.18104672 0.00833761 0.05898831] \n",
            "Epoch: 40 | Loss: [0.17844492 0.00821778 0.0581404 ] \n",
            "Epoch: 41 | Loss: [0.17588054 0.00809968 0.05730478] \n",
            "Epoch: 42 | Loss: [0.1733528  0.00798338 0.05648088] \n",
            "Epoch: 43 | Loss: [0.17086147 0.00786859 0.05566925] \n",
            "Epoch: 44 | Loss: [0.16840598 0.00775555 0.0548693 ] \n",
            "Epoch: 45 | Loss: [0.16598575 0.00764409 0.05408068] \n",
            "Epoch: 46 | Loss: [0.16360019 0.00753426 0.05330327] \n",
            "Epoch: 47 | Loss: [0.16124895 0.00742596 0.0525374 ] \n",
            "Epoch: 48 | Loss: [0.15893166 0.00731918 0.05178249] \n",
            "Epoch: 49 | Loss: [0.15664753 0.00721407 0.05103821] \n",
            "Epoch: 50 | Loss: [0.15439625 0.00711036 0.05030446] \n",
            "Epoch: 51 | Loss: [0.15217745 0.00700819 0.04958154] \n",
            "Epoch: 52 | Loss: [0.14999041 0.00690749 0.04886912] \n",
            "Epoch: 53 | Loss: [0.14783476 0.00680821 0.04816667] \n",
            "Epoch: 54 | Loss: [0.14571021 0.00671036 0.04747429] \n",
            "Epoch: 55 | Loss: [0.1436162  0.00661392 0.04679229] \n",
            "Epoch: 56 | Loss: [0.14155206 0.00651887 0.04611973] \n",
            "Epoch: 57 | Loss: [0.13951783 0.00642519 0.04545692] \n",
            "Epoch: 58 | Loss: [0.13751282 0.00633287 0.04480355] \n",
            "Epoch: 59 | Loss: [0.13553636 0.00624183 0.04415992] \n",
            "Epoch: 60 | Loss: [0.1335885  0.00615212 0.04352512] \n",
            "Epoch: 61 | Loss: [0.13166857 0.00606372 0.04289946] \n",
            "Epoch: 62 | Loss: [0.12977628 0.00597655 0.04228304] \n",
            "Epoch: 63 | Loss: [0.12791133 0.00589068 0.04167536] \n",
            "Epoch: 64 | Loss: [0.12607293 0.00580601 0.04107652] \n",
            "Epoch: 65 | Loss: [0.1242611  0.00572259 0.04048605] \n",
            "Epoch: 66 | Loss: [0.12247524 0.00564029 0.03990423] \n",
            "Epoch: 67 | Loss: [0.12071505 0.00555922 0.03933079] \n",
            "Epoch: 68 | Loss: [0.11898028 0.00547937 0.03876562] \n",
            "Epoch: 69 | Loss: [0.11727031 0.00540059 0.03820847] \n",
            "Epoch: 70 | Loss: [0.11558487 0.00532301 0.03765922] \n",
            "Epoch: 71 | Loss: [0.11392386 0.00524647 0.03711819] \n",
            "Epoch: 72 | Loss: [0.11228669 0.0051711  0.03658453] \n",
            "Epoch: 73 | Loss: [0.11067296 0.00509683 0.03605872] \n",
            "Epoch: 74 | Loss: [0.10908224 0.00502356 0.03554067] \n",
            "Epoch: 75 | Loss: [0.10751461 0.00495136 0.03502977] \n",
            "Epoch: 76 | Loss: [0.1059695  0.00488022 0.03452627] \n",
            "Epoch: 77 | Loss: [0.10444652 0.00481005 0.03403012] \n",
            "Epoch: 78 | Loss: [0.10294542 0.00474092 0.03354106] \n",
            "Epoch: 79 | Loss: [0.10146597 0.00467281 0.033059  ] \n",
            "Epoch: 80 | Loss: [0.10000778 0.00460564 0.03258388] \n",
            "Epoch: 81 | Loss: [0.09857048 0.00453948 0.03211561] \n",
            "Epoch: 82 | Loss: [0.09715383 0.00447424 0.03165429] \n",
            "Epoch: 83 | Loss: [0.09575763 0.00440992 0.03119917] \n",
            "Epoch: 84 | Loss: [0.09438133 0.0043465  0.03075087] \n",
            "Epoch: 85 | Loss: [0.09302489 0.00428404 0.03030896] \n",
            "Epoch: 86 | Loss: [0.09168809 0.00422247 0.02987338] \n",
            "Epoch: 87 | Loss: [0.09037028 0.00416177 0.02944406] \n",
            "Epoch: 88 | Loss: [0.08907154 0.004102   0.02902094] \n",
            "Epoch: 89 | Loss: [0.08779153 0.00404303 0.02860378] \n",
            "Epoch: 90 | Loss: [0.08652976 0.00398496 0.02819268] \n",
            "Epoch: 91 | Loss: [0.08528617 0.00392768 0.02778758] \n",
            "Epoch: 92 | Loss: [0.08406043 0.00387122 0.02738825] \n",
            "Epoch: 93 | Loss: [0.08285235 0.00381559 0.02699448] \n",
            "Epoch: 94 | Loss: [0.08166173 0.00376076 0.02660667] \n",
            "Epoch: 95 | Loss: [0.08048812 0.00370669 0.02622413] \n",
            "Epoch: 96 | Loss: [0.07933132 0.0036534  0.02584728] \n",
            "Epoch: 97 | Loss: [0.07819118 0.00360091 0.0254759 ] \n",
            "Epoch: 98 | Loss: [0.07706748 0.00354913 0.02510977] \n",
            "Epoch: 99 | Loss: [0.07595994 0.00349818 0.02474884] \n",
            "Epoch: 100 | Loss: [0.07486823 0.00344788 0.0243932 ] \n",
            "Epoch: 101 | Loss: [0.07379221 0.00339834 0.02404266] \n",
            "Epoch: 102 | Loss: [0.07273169 0.00334948 0.02369715] \n",
            "Epoch: 103 | Loss: [0.0716865  0.00330136 0.02335646] \n",
            "Epoch: 104 | Loss: [0.07065623 0.00325391 0.02302085] \n",
            "Epoch: 105 | Loss: [0.06964084 0.00320713 0.02268997] \n",
            "Epoch: 106 | Loss: [0.06863992 0.00316107 0.02236391] \n",
            "Epoch: 107 | Loss: [0.06765344 0.0031156  0.02204247] \n",
            "Epoch: 108 | Loss: [0.06668124 0.00307084 0.02172575] \n",
            "Epoch: 109 | Loss: [0.06572292 0.00302676 0.02141327] \n",
            "Epoch: 110 | Loss: [0.06477834 0.00298322 0.02110583] \n",
            "Epoch: 111 | Loss: [0.06384733 0.00294035 0.02080254] \n",
            "Epoch: 112 | Loss: [0.06292977 0.0028981  0.02050349] \n",
            "Epoch: 113 | Loss: [0.06202538 0.0028564  0.02020891] \n",
            "Epoch: 114 | Loss: [0.06113402 0.00281537 0.01991834] \n",
            "Epoch: 115 | Loss: [0.06025543 0.00277493 0.01963215] \n",
            "Epoch: 116 | Loss: [0.05938947 0.00273504 0.01934989] \n",
            "Epoch: 117 | Loss: [0.0585359  0.00269573 0.01907191] \n",
            "Epoch: 118 | Loss: [0.05769468 0.002657   0.01879778] \n",
            "Epoch: 119 | Loss: [0.05686548 0.0026188  0.01852757] \n",
            "Epoch: 120 | Loss: [0.05604825 0.00258116 0.01826139] \n",
            "Epoch: 121 | Loss: [0.05524277 0.00254408 0.01799892] \n",
            "Epoch: 122 | Loss: [0.05444879 0.00250751 0.01774025] \n",
            "Epoch: 123 | Loss: [0.0536663  0.00247149 0.01748535] \n",
            "Epoch: 124 | Loss: [0.05289507 0.00243597 0.01723405] \n",
            "Epoch: 125 | Loss: [0.05213487 0.00240093 0.01698643] \n",
            "Epoch: 126 | Loss: [0.05138557 0.00236643 0.01674221] \n",
            "Epoch: 127 | Loss: [0.05064717 0.00233246 0.01650146] \n",
            "Epoch: 128 | Loss: [0.04991923 0.00229891 0.01626441] \n",
            "Epoch: 129 | Loss: [0.04920184 0.00226588 0.01603064] \n",
            "Epoch: 130 | Loss: [0.04849468 0.00223331 0.01580025] \n",
            "Epoch: 131 | Loss: [0.04779775 0.0022012  0.01557319] \n",
            "Epoch: 132 | Loss: [0.04711084 0.00216959 0.01534942] \n",
            "Epoch: 133 | Loss: [0.04643372 0.00213839 0.01512892] \n",
            "Epoch: 134 | Loss: [0.04576641 0.00210767 0.01491141] \n",
            "Epoch: 135 | Loss: [0.04510868 0.0020774  0.01469697] \n",
            "Epoch: 136 | Loss: [0.04446044 0.00204751 0.01448593] \n",
            "Epoch: 137 | Loss: [0.04382148 0.0020181  0.01427767] \n",
            "Epoch: 138 | Loss: [0.0431916  0.00198908 0.0140725 ] \n",
            "Epoch: 139 | Loss: [0.04257092 0.00196051 0.01387027] \n",
            "Epoch: 140 | Loss: [0.04195911 0.00193232 0.01367084] \n",
            "Epoch: 141 | Loss: [0.0413561  0.00190459 0.01347441] \n",
            "Epoch: 142 | Loss: [0.04076169 0.00187718 0.01328083] \n",
            "Epoch: 143 | Loss: [0.04017588 0.00185021 0.01308996] \n",
            "Epoch: 144 | Loss: [0.03959849 0.0018236  0.01290177] \n",
            "Epoch: 145 | Loss: [0.03902942 0.00179743 0.01271634] \n",
            "Epoch: 146 | Loss: [0.03846858 0.0017716  0.01253364] \n",
            "Epoch: 147 | Loss: [0.0379157  0.00174613 0.01235343] \n",
            "Epoch: 148 | Loss: [0.03737079 0.00172103 0.01217599] \n",
            "Epoch: 149 | Loss: [0.03683367 0.00169627 0.01200099] \n",
            "Epoch: 150 | Loss: [0.03630434 0.00167189 0.01182861] \n",
            "Epoch: 151 | Loss: [0.03578263 0.00164788 0.0116585 ] \n",
            "Epoch: 152 | Loss: [0.03526837 0.0016242  0.01149095] \n",
            "Epoch: 153 | Loss: [0.03476147 0.00160087 0.01132583] \n",
            "Epoch: 154 | Loss: [0.03426195 0.00157787 0.01116302] \n",
            "Epoch: 155 | Loss: [0.03376955 0.00155519 0.01100248] \n",
            "Epoch: 156 | Loss: [0.03328419 0.00153282 0.0108445 ] \n",
            "Epoch: 157 | Loss: [0.03280589 0.00151083 0.01068855] \n",
            "Epoch: 158 | Loss: [0.0323344  0.00148911 0.010535  ] \n",
            "Epoch: 159 | Loss: [0.03186972 0.0014677  0.01038353] \n",
            "Epoch: 160 | Loss: [0.03141161 0.00144662 0.01023432] \n",
            "Epoch: 161 | Loss: [0.03096018 0.0014258  0.01008734] \n",
            "Epoch: 162 | Loss: [0.03051527 0.00140531 0.00994227] \n",
            "Epoch: 163 | Loss: [0.03007671 0.00138512 0.00979939] \n",
            "Epoch: 164 | Loss: [0.02964445 0.00136521 0.00965857] \n",
            "Epoch: 165 | Loss: [0.02921842 0.00134561 0.00951971] \n",
            "Epoch: 166 | Loss: [0.02879846 0.00132623 0.00938295] \n",
            "Epoch: 167 | Loss: [0.0283846  0.0013072  0.00924811] \n",
            "Epoch: 168 | Loss: [0.02797668 0.00128841 0.00911524] \n",
            "Epoch: 169 | Loss: [0.02757457 0.00126989 0.00898424] \n",
            "Epoch: 170 | Loss: [0.02717827 0.00125164 0.00885508] \n",
            "Epoch: 171 | Loss: [0.02678773 0.00123362 0.00872793] \n",
            "Epoch: 172 | Loss: [0.02640273 0.00121593 0.00860249] \n",
            "Epoch: 173 | Loss: [0.02602329 0.00119843 0.00847875] \n",
            "Epoch: 174 | Loss: [0.02564934 0.00118123 0.00835695] \n",
            "Epoch: 175 | Loss: [0.02528067 0.00116425 0.00823681] \n",
            "Epoch: 176 | Loss: [0.02491738 0.00114751 0.00811841] \n",
            "Epoch: 177 | Loss: [0.02455927 0.00113103 0.00800179] \n",
            "Epoch: 178 | Loss: [0.02420634 0.00111477 0.0078867 ] \n",
            "Epoch: 179 | Loss: [0.02385839 0.00109875 0.00777345] \n",
            "Epoch: 180 | Loss: [0.02351553 0.00108293 0.00766178] \n",
            "Epoch: 181 | Loss: [0.02317761 0.00106739 0.00755157] \n",
            "Epoch: 182 | Loss: [0.02284452 0.00105206 0.00744298] \n",
            "Epoch: 183 | Loss: [0.0225162  0.00103692 0.00733616] \n",
            "Epoch: 184 | Loss: [0.0221926  0.00102202 0.00723068] \n",
            "Epoch: 185 | Loss: [0.02187367 0.00100735 0.00712677] \n",
            "Epoch: 186 | Loss: [0.02155928 0.00099287 0.00702433] \n",
            "Epoch: 187 | Loss: [0.02124947 0.00097859 0.00692335] \n",
            "Epoch: 188 | Loss: [0.02094404 0.00096453 0.00682388] \n",
            "Epoch: 189 | Loss: [0.02064301 0.00095066 0.00672584] \n",
            "Epoch: 190 | Loss: [0.02034634 0.00093701 0.00662921] \n",
            "Epoch: 191 | Loss: [0.02005397 0.00092354 0.00653381] \n",
            "Epoch: 192 | Loss: [0.01976574 0.00091026 0.00643995] \n",
            "Epoch: 193 | Loss: [0.01948171 0.00089718 0.00634738] \n",
            "Epoch: 194 | Loss: [0.01920173 0.00088429 0.00625615] \n",
            "Epoch: 195 | Loss: [0.01892574 0.00087157 0.00616626] \n",
            "Epoch: 196 | Loss: [0.01865377 0.00085906 0.00607762] \n",
            "Epoch: 197 | Loss: [0.01838564 0.0008467  0.00599035] \n",
            "Epoch: 198 | Loss: [0.01812144 0.00083456 0.00590423] \n",
            "Epoch: 199 | Loss: [0.017861   0.00082253 0.00581931] \n",
            "Epoch: 200 | Loss: [0.01760428 0.00081073 0.00573573] \n",
            "Epoch: 201 | Loss: [0.01735131 0.00079907 0.00565333] \n",
            "Epoch: 202 | Loss: [0.01710197 0.00078758 0.0055721 ] \n",
            "Epoch: 203 | Loss: [0.01685617 0.00077627 0.00549194] \n",
            "Epoch: 204 | Loss: [0.01661393 0.00076512 0.005413  ] \n",
            "Epoch: 205 | Loss: [0.01637516 0.00075411 0.00533533] \n",
            "Epoch: 206 | Loss: [0.01613981 0.00074328 0.00525863] \n",
            "Epoch: 207 | Loss: [0.01590784 0.00073261 0.00518304] \n",
            "Epoch: 208 | Loss: [0.01567923 0.00072209 0.00510848] \n",
            "Epoch: 209 | Loss: [0.01545393 0.00071169 0.00503506] \n",
            "Epoch: 210 | Loss: [0.01523179 0.00070145 0.00496278] \n",
            "Epoch: 211 | Loss: [0.0150129  0.00069139 0.00489148] \n",
            "Epoch: 212 | Loss: [0.01479715 0.00068144 0.0048211 ] \n",
            "Epoch: 213 | Loss: [0.01458447 0.00067164 0.00475183] \n",
            "Epoch: 214 | Loss: [0.01437486 0.00066199 0.00468357] \n",
            "Epoch: 215 | Loss: [0.01416825 0.00065248 0.00461626] \n",
            "Epoch: 216 | Loss: [0.01396466 0.00064311 0.00454989] \n",
            "Epoch: 217 | Loss: [0.013764   0.00063388 0.00448452] \n",
            "Epoch: 218 | Loss: [0.01356617 0.00062477 0.00442006] \n",
            "Epoch: 219 | Loss: [0.01337121 0.00061579 0.00435656] \n",
            "Epoch: 220 | Loss: [0.01317903 0.00060692 0.00429391] \n",
            "Epoch: 221 | Loss: [0.0129896  0.00059821 0.0042322 ] \n",
            "Epoch: 222 | Loss: [0.01280295 0.00058961 0.00417143] \n",
            "Epoch: 223 | Loss: [0.01261893 0.00058112 0.00411147] \n",
            "Epoch: 224 | Loss: [0.01243757 0.00057279 0.00405237] \n",
            "Epoch: 225 | Loss: [0.01225885 0.00056455 0.00399412] \n",
            "Epoch: 226 | Loss: [0.01208267 0.00055645 0.00393671] \n",
            "Epoch: 227 | Loss: [0.01190902 0.00054845 0.00388013] \n",
            "Epoch: 228 | Loss: [0.01173787 0.00054057 0.00382437] \n",
            "Epoch: 229 | Loss: [0.01156919 0.00053279 0.00376937] \n",
            "Epoch: 230 | Loss: [0.0114029  0.00052514 0.00371529] \n",
            "Epoch: 231 | Loss: [0.01123902 0.00051758 0.00366183] \n",
            "Epoch: 232 | Loss: [0.01107749 0.00051014 0.00360921] \n",
            "Epoch: 233 | Loss: [0.01091832 0.00050282 0.00355738] \n",
            "Epoch: 234 | Loss: [0.01076139 0.0004956  0.0035062 ] \n",
            "Epoch: 235 | Loss: [0.01060673 0.00048847 0.00345584] \n",
            "Epoch: 236 | Loss: [0.0104543  0.00048144 0.00340624] \n",
            "Epoch: 237 | Loss: [0.0103041  0.00047454 0.00335721] \n",
            "Epoch: 238 | Loss: [0.01015599 0.0004677  0.00330898] \n",
            "Epoch: 239 | Loss: [0.01001    0.00046098 0.00326142] \n",
            "Epoch: 240 | Loss: [0.00986615 0.00045437 0.00321454] \n",
            "Epoch: 241 | Loss: [0.00972434 0.00044783 0.00316837] \n",
            "Epoch: 242 | Loss: [0.00958463 0.0004414  0.0031228 ] \n",
            "Epoch: 243 | Loss: [0.0094469  0.00043507 0.00307792] \n",
            "Epoch: 244 | Loss: [0.00931108 0.00042881 0.00303369] \n",
            "Epoch: 245 | Loss: [0.00917731 0.00042265 0.00299004] \n",
            "Epoch: 246 | Loss: [0.00904536 0.00041655 0.00294713] \n",
            "Epoch: 247 | Loss: [0.0089154  0.00041058 0.00290477] \n",
            "Epoch: 248 | Loss: [0.00878727 0.00040467 0.00286309] \n",
            "Epoch: 249 | Loss: [0.00866097 0.00039886 0.0028219 ] \n",
            "Epoch: 250 | Loss: [0.0085365  0.00039312 0.00278132] \n",
            "Epoch: 251 | Loss: [0.0084138  0.00038747 0.00274138] \n",
            "Epoch: 252 | Loss: [0.00829291 0.00038192 0.00270197] \n",
            "Epoch: 253 | Loss: [0.00817375 0.00037642 0.00266315] \n",
            "Epoch: 254 | Loss: [0.00805627 0.00037102 0.00262485] \n",
            "Epoch: 255 | Loss: [0.00794049 0.00036569 0.00258712] \n",
            "Epoch: 256 | Loss: [0.00782639 0.00036044 0.0025499 ] \n",
            "Epoch: 257 | Loss: [0.00771387 0.00035524 0.00251329] \n",
            "Epoch: 258 | Loss: [0.00760304 0.00035014 0.00247714] \n",
            "Epoch: 259 | Loss: [0.00749375 0.00034511 0.00244157] \n",
            "Epoch: 260 | Loss: [0.00738607 0.00034015 0.00240645] \n",
            "Epoch: 261 | Loss: [0.00727991 0.00033526 0.00237191] \n",
            "Epoch: 262 | Loss: [0.00717528 0.00033044 0.0023378 ] \n",
            "Epoch: 263 | Loss: [0.00707217 0.00032569 0.00230422] \n",
            "Epoch: 264 | Loss: [0.00697052 0.00032101 0.0022711 ] \n",
            "Epoch: 265 | Loss: [0.00687036 0.00031639 0.0022385 ] \n",
            "Epoch: 266 | Loss: [0.00677163 0.00031186 0.00220626] \n",
            "Epoch: 267 | Loss: [0.00667432 0.00030738 0.00217457] \n",
            "Epoch: 268 | Loss: [0.00657837 0.00030295 0.00214329] \n",
            "Epoch: 269 | Loss: [0.00648384 0.0002986  0.00211249] \n",
            "Epoch: 270 | Loss: [0.00639065 0.00029432 0.00208214] \n",
            "Epoch: 271 | Loss: [0.00629881 0.00029008 0.00205222] \n",
            "Epoch: 272 | Loss: [0.00620827 0.00028592 0.00202269] \n",
            "Epoch: 273 | Loss: [0.00611906 0.00028181 0.00199367] \n",
            "Epoch: 274 | Loss: [0.00603113 0.00027775 0.00196503] \n",
            "Epoch: 275 | Loss: [0.00594442 0.00027376 0.00193677] \n",
            "Epoch: 276 | Loss: [0.00585899 0.00026982 0.00190896] \n",
            "Epoch: 277 | Loss: [0.0057748  0.00026595 0.00188152] \n",
            "Epoch: 278 | Loss: [0.00569179 0.00026212 0.00185448] \n",
            "Epoch: 279 | Loss: [0.00561    0.00025836 0.0018278 ] \n",
            "Epoch: 280 | Loss: [0.00552939 0.00025465 0.00180155] \n",
            "Epoch: 281 | Loss: [0.0054499  0.00025098 0.00177566] \n",
            "Epoch: 282 | Loss: [0.00537161 0.00024738 0.00175011] \n",
            "Epoch: 283 | Loss: [0.00529438 0.00024383 0.00172495] \n",
            "Epoch: 284 | Loss: [0.00521829 0.00024031 0.0017002 ] \n",
            "Epoch: 285 | Loss: [0.0051433  0.00023686 0.00167579] \n",
            "Epoch: 286 | Loss: [0.00506939 0.00023346 0.00165168] \n",
            "Epoch: 287 | Loss: [0.00499653 0.0002301  0.00162793] \n",
            "Epoch: 288 | Loss: [0.00492472 0.0002268  0.00160454] \n",
            "Epoch: 289 | Loss: [0.00485394 0.00022354 0.00158151] \n",
            "Epoch: 290 | Loss: [0.00478419 0.00022033 0.00155876] \n",
            "Epoch: 291 | Loss: [0.00471541 0.00021715 0.0015364 ] \n",
            "Epoch: 292 | Loss: [0.00464765 0.00021403 0.00151428] \n",
            "Epoch: 293 | Loss: [0.00458086 0.00021096 0.00149254] \n",
            "Epoch: 294 | Loss: [0.00451503 0.00020792 0.00147106] \n",
            "Epoch: 295 | Loss: [0.00445016 0.00020494 0.00144992] \n",
            "Epoch: 296 | Loss: [0.0043862  0.000202   0.00142908] \n",
            "Epoch: 297 | Loss: [0.00432317 0.00019909 0.00140853] \n",
            "Epoch: 298 | Loss: [0.00426104 0.00019624 0.00138831] \n",
            "Epoch: 299 | Loss: [0.00419982 0.00019342 0.00136831] \n",
            "Epoch: 300 | Loss: [0.00413944 0.00019064 0.00134866] \n",
            "Epoch: 301 | Loss: [0.00407995 0.0001879  0.00132929] \n",
            "Epoch: 302 | Loss: [0.00402132 0.00018519 0.0013102 ] \n",
            "Epoch: 303 | Loss: [0.00396353 0.00018254 0.00129135] \n",
            "Epoch: 304 | Loss: [0.00390655 0.00017991 0.00127281] \n",
            "Epoch: 305 | Loss: [0.00385042 0.00017732 0.0012545 ] \n",
            "Epoch: 306 | Loss: [0.00379506 0.00017478 0.0012365 ] \n",
            "Epoch: 307 | Loss: [0.00374053 0.00017226 0.00121872] \n",
            "Epoch: 308 | Loss: [0.00368677 0.00016978 0.00120121] \n",
            "Epoch: 309 | Loss: [0.00363377 0.00016734 0.00118395] \n",
            "Epoch: 310 | Loss: [0.00358156 0.00016493 0.00116695] \n",
            "Epoch: 311 | Loss: [0.0035301  0.00016257 0.00115017] \n",
            "Epoch: 312 | Loss: [0.00347937 0.00016024 0.00113363] \n",
            "Epoch: 313 | Loss: [0.00342937 0.00015793 0.00111732] \n",
            "Epoch: 314 | Loss: [0.00338007 0.00015566 0.00110128] \n",
            "Epoch: 315 | Loss: [0.00333148 0.00015342 0.00108545] \n",
            "Epoch: 316 | Loss: [0.00328363 0.00015122 0.00106982] \n",
            "Epoch: 317 | Loss: [0.00323642 0.00014905 0.0010545 ] \n",
            "Epoch: 318 | Loss: [0.0031899  0.0001469  0.00103932] \n",
            "Epoch: 319 | Loss: [0.00314407 0.00014479 0.0010244 ] \n",
            "Epoch: 320 | Loss: [0.00309889 0.00014271 0.00100968] \n",
            "Epoch: 321 | Loss: [0.00305435 0.00014066 0.00099516] \n",
            "Epoch: 322 | Loss: [0.00301044 0.00013864 0.00098086] \n",
            "Epoch: 323 | Loss: [0.0029672  0.00013665 0.00096675] \n",
            "Epoch: 324 | Loss: [0.00292454 0.00013468 0.00095287] \n",
            "Epoch: 325 | Loss: [0.00288251 0.00013275 0.00093914] \n",
            "Epoch: 326 | Loss: [0.00284108 0.00013084 0.00092566] \n",
            "Epoch: 327 | Loss: [0.00280026 0.00012896 0.00091239] \n",
            "Epoch: 328 | Loss: [0.00276003 0.00012711 0.00089921] \n",
            "Epoch: 329 | Loss: [0.00272034 0.00012528 0.0008863 ] \n",
            "Epoch: 330 | Loss: [0.00268127 0.00012348 0.00087357] \n",
            "Epoch: 331 | Loss: [0.00264271 0.00012171 0.00086105] \n",
            "Epoch: 332 | Loss: [0.00260473 0.00011996 0.00084867] \n",
            "Epoch: 333 | Loss: [0.00256732 0.00011824 0.00083646] \n",
            "Epoch: 334 | Loss: [0.00253041 0.00011653 0.00082445] \n",
            "Epoch: 335 | Loss: [0.00249404 0.00011485 0.00081261] \n",
            "Epoch: 336 | Loss: [0.00245819 0.0001132  0.00080091] \n",
            "Epoch: 337 | Loss: [0.00242288 0.00011158 0.0007894 ] \n",
            "Epoch: 338 | Loss: [0.00238805 0.00010998 0.00077805] \n",
            "Epoch: 339 | Loss: [0.00235374 0.00010839 0.00076686] \n",
            "Epoch: 340 | Loss: [0.0023199  0.00010684 0.00075587] \n",
            "Epoch: 341 | Loss: [0.00228656 0.00010531 0.00074497] \n",
            "Epoch: 342 | Loss: [0.0022537  0.00010379 0.00073429] \n",
            "Epoch: 343 | Loss: [0.00222131 0.00010231 0.0007237 ] \n",
            "Epoch: 344 | Loss: [0.00218938 0.00010083 0.00071332] \n",
            "Epoch: 345 | Loss: [2.1579254e-03 9.9376128e-05 7.0307089e-04] \n",
            "Epoch: 346 | Loss: [2.1269158e-03 9.7955199e-05 6.9296733e-04] \n",
            "Epoch: 347 | Loss: [2.0963273e-03 9.6544507e-05 6.8301160e-04] \n",
            "Epoch: 348 | Loss: [2.0661985e-03 9.5153351e-05 6.7322695e-04] \n",
            "Epoch: 349 | Loss: [2.036503e-03 9.378152e-05 6.635374e-04] \n",
            "Epoch: 350 | Loss: [2.0072362e-03 9.2437986e-05 6.5401569e-04] \n",
            "Epoch: 351 | Loss: [1.9783932e-03 9.1113252e-05 6.4458698e-04] \n",
            "Epoch: 352 | Loss: [1.9499694e-03 8.9798079e-05 6.3532288e-04] \n",
            "Epoch: 353 | Loss: [1.9219395e-03 8.8510409e-05 6.2622130e-04] \n",
            "Epoch: 354 | Loss: [1.8943201e-03 8.7232038e-05 6.1723276e-04] \n",
            "Epoch: 355 | Loss: [1.8670860e-03 8.5980653e-05 6.0833275e-04] \n",
            "Epoch: 356 | Loss: [1.8402537e-03 8.4747087e-05 5.9961405e-04] \n",
            "Epoch: 357 | Loss: [1.8138186e-03 8.3531151e-05 5.9098157e-04] \n",
            "Epoch: 358 | Loss: [1.7877562e-03 8.2332655e-05 5.8248063e-04] \n",
            "Epoch: 359 | Loss: [1.7620625e-03 8.1151404e-05 5.7410990e-04] \n",
            "Epoch: 360 | Loss: [1.7367337e-03 7.9978694e-05 5.6584511e-04] \n",
            "Epoch: 361 | Loss: [1.7117658e-03 7.8831450e-05 5.5773027e-04] \n",
            "Epoch: 362 | Loss: [1.6871745e-03 7.7700905e-05 5.4971880e-04] \n",
            "Epoch: 363 | Loss: [1.6629361e-03 7.6586868e-05 5.4180971e-04] \n",
            "Epoch: 364 | Loss: [1.6390275e-03 7.5480872e-05 5.3400197e-04] \n",
            "Epoch: 365 | Loss: [1.6154838e-03 7.4399381e-05 5.2633841e-04] \n",
            "Epoch: 366 | Loss: [1.5922625e-03 7.3325682e-05 5.1879539e-04] \n",
            "Epoch: 367 | Loss: [1.5693794e-03 7.2276009e-05 5.1132840e-04] \n",
            "Epoch: 368 | Loss: [1.5468118e-03 7.1233895e-05 5.0397974e-04] \n",
            "Epoch: 369 | Loss: [1.5245939e-03 7.0215341e-05 4.9672683e-04] \n",
            "Epoch: 370 | Loss: [1.5026847e-03 6.9204114e-05 4.8958976e-04] \n",
            "Epoch: 371 | Loss: [1.4810808e-03 6.8208101e-05 4.8256715e-04] \n",
            "Epoch: 372 | Loss: [1.4597973e-03 6.7227127e-05 4.7561608e-04] \n",
            "Epoch: 373 | Loss: [1.4388124e-03 6.6261018e-05 4.6877741e-04] \n",
            "Epoch: 374 | Loss: [1.4181412e-03 6.5309607e-05 4.6204973e-04] \n",
            "Epoch: 375 | Loss: [1.397762e-03 6.437273e-05 4.554114e-04] \n",
            "Epoch: 376 | Loss: [1.3776543e-03 6.3442625e-05 4.4888171e-04] \n",
            "Epoch: 377 | Loss: [1.3578680e-03 6.2534367e-05 4.4241923e-04] \n",
            "Epoch: 378 | Loss: [1.3383469e-03 6.1632658e-05 4.3606333e-04] \n",
            "Epoch: 379 | Loss: [1.3191230e-03 6.0744929e-05 4.2981276e-04] \n",
            "Epoch: 380 | Loss: [1.3001584e-03 5.9878399e-05 4.2360730e-04] \n",
            "Epoch: 381 | Loss: [1.2814678e-03 5.9010767e-05 4.1752489e-04] \n",
            "Epoch: 382 | Loss: [1.2630651e-03 5.8164012e-05 4.1152516e-04] \n",
            "Epoch: 383 | Loss: [1.2449132e-03 5.7330599e-05 4.0560728e-04] \n",
            "Epoch: 384 | Loss: [1.2270096e-03 5.6503199e-05 3.9978945e-04] \n",
            "Epoch: 385 | Loss: [1.2093849e-03 5.5696048e-05 3.9403260e-04] \n",
            "Epoch: 386 | Loss: [1.1920029e-03 5.4894699e-05 3.8837385e-04] \n",
            "Epoch: 387 | Loss: [1.1748774e-03 5.4106175e-05 3.8279334e-04] \n",
            "Epoch: 388 | Loss: [1.1579895e-03 5.3330321e-05 3.7729030e-04] \n",
            "Epoch: 389 | Loss: [1.1413528e-03 5.2560066e-05 3.7186383e-04] \n",
            "Epoch: 390 | Loss: [1.1249483e-03 5.1809144e-05 3.6653149e-04] \n",
            "Epoch: 391 | Loss: [1.1087738e-03 5.1063624e-05 3.6125575e-04] \n",
            "Epoch: 392 | Loss: [1.0928423e-03 5.0330273e-05 3.5607224e-04] \n",
            "Epoch: 393 | Loss: [1.0771359e-03 4.9608945e-05 3.5092622e-04] \n",
            "Epoch: 394 | Loss: [1.0616517e-03 4.8892820e-05 3.4590630e-04] \n",
            "Epoch: 395 | Loss: [1.0463878e-03 4.8188522e-05 3.4094017e-04] \n",
            "Epoch: 396 | Loss: [1.0313567e-03 4.7495909e-05 3.3604490e-04] \n",
            "Epoch: 397 | Loss: [1.0165409e-03 4.6814832e-05 3.3120241e-04] \n",
            "Epoch: 398 | Loss: [1.0019229e-03 4.6138673e-05 3.2644672e-04] \n",
            "Epoch: 399 | Loss: [9.8753057e-04 4.5480294e-05 3.2175967e-04] \n",
            "Epoch: 400 | Loss: [9.7333163e-04 4.4826647e-05 3.1712346e-04] \n",
            "Epoch: 401 | Loss: [9.5933885e-04 4.4177734e-05 3.1258835e-04] \n",
            "Epoch: 402 | Loss: [9.4556471e-04 4.3546133e-05 3.0808590e-04] \n",
            "Epoch: 403 | Loss: [9.3197753e-04 4.2919084e-05 3.0364934e-04] \n",
            "Epoch: 404 | Loss: [9.1857539e-04 4.2302781e-05 2.9929445e-04] \n",
            "Epoch: 405 | Loss: [9.0538507e-04 4.1697094e-05 2.9498740e-04] \n",
            "Epoch: 406 | Loss: [8.9237560e-04 4.1095776e-05 2.9074409e-04] \n",
            "Epoch: 407 | Loss: [8.7954511e-04 4.0504896e-05 2.8656382e-04] \n",
            "Epoch: 408 | Loss: [8.6690579e-04 3.9924318e-05 2.8244586e-04] \n",
            "Epoch: 409 | Loss: [8.5444161e-04 3.9347931e-05 2.7838952e-04] \n",
            "Epoch: 410 | Loss: [8.4216450e-04 3.8781673e-05 2.7439411e-04] \n",
            "Epoch: 411 | Loss: [8.3005871e-04 3.8225415e-05 2.7044327e-04] \n",
            "Epoch: 412 | Loss: [8.1813603e-04 3.7679030e-05 2.6655223e-04] \n",
            "Epoch: 413 | Loss: [8.0636726e-04 3.7136579e-05 2.6273576e-04] \n",
            "Epoch: 414 | Loss: [7.947779e-04 3.660383e-05 2.589468e-04] \n",
            "Epoch: 415 | Loss: [7.833524e-04 3.607493e-05 2.552463e-04] \n",
            "Epoch: 416 | Loss: [7.7210244e-04 3.5555564e-05 2.5155730e-04] \n",
            "Epoch: 417 | Loss: [7.609996e-04 3.504561e-05 2.479552e-04] \n",
            "Epoch: 418 | Loss: [7.5006863e-04 3.4539342e-05 2.4439400e-04] \n",
            "Epoch: 419 | Loss: [7.3929445e-04 3.4047884e-05 2.4087337e-04] \n",
            "Epoch: 420 | Loss: [7.2866265e-04 3.3559947e-05 2.3739297e-04] \n",
            "Epoch: 421 | Loss: [7.1819720e-04 3.3075532e-05 2.3399625e-04] \n",
            "Epoch: 422 | Loss: [7.0787099e-04 3.2600081e-05 2.3063848e-04] \n",
            "Epoch: 423 | Loss: [6.9769507e-04 3.2128075e-05 2.2731937e-04] \n",
            "Epoch: 424 | Loss: [6.8766781e-04 3.1664877e-05 2.2406713e-04] \n",
            "Epoch: 425 | Loss: [6.7778764e-04 3.1210369e-05 2.2083832e-04] \n",
            "Epoch: 426 | Loss: [6.6804059e-04 3.0764437e-05 2.1767516e-04] \n",
            "Epoch: 427 | Loss: [6.5844978e-04 3.0321713e-05 2.1453481e-04] \n",
            "Epoch: 428 | Loss: [6.4897689e-04 2.9887411e-05 2.1145889e-04] \n",
            "Epoch: 429 | Loss: [6.3965708e-04 2.9456241e-05 2.0840517e-04] \n",
            "Epoch: 430 | Loss: [6.3046452e-04 2.9033345e-05 2.0541466e-04] \n",
            "Epoch: 431 | Loss: [6.2140980e-04 2.8618606e-05 2.0245934e-04] \n",
            "Epoch: 432 | Loss: [6.1246782e-04 2.8206850e-05 1.9955238e-04] \n",
            "Epoch: 433 | Loss: [6.0367258e-04 2.7798080e-05 1.9669317e-04] \n",
            "Epoch: 434 | Loss: [5.9499918e-04 2.7402275e-05 1.9385462e-04] \n",
            "Epoch: 435 | Loss: [5.8644626e-04 2.7009310e-05 1.9107622e-04] \n",
            "Epoch: 436 | Loss: [5.7802402e-04 2.6619182e-05 1.8831789e-04] \n",
            "Epoch: 437 | Loss: [5.6970818e-04 2.6236776e-05 1.8561858e-04] \n",
            "Epoch: 438 | Loss: [5.6152046e-04 2.5861988e-05 1.8295167e-04] \n",
            "Epoch: 439 | Loss: [5.5344804e-04 2.5489895e-05 1.8031686e-04] \n",
            "Epoch: 440 | Loss: [5.4550089e-04 2.5120498e-05 1.7772659e-04] \n",
            "Epoch: 441 | Loss: [5.3766655e-04 2.4763289e-05 1.7516766e-04] \n",
            "Epoch: 442 | Loss: [5.2993273e-04 2.4403926e-05 1.7265236e-04] \n",
            "Epoch: 443 | Loss: [5.2232033e-04 2.4056542e-05 1.7018014e-04] \n",
            "Epoch: 444 | Loss: [5.1480625e-04 2.3707007e-05 1.6772574e-04] \n",
            "Epoch: 445 | Loss: [5.0741108e-04 2.3369248e-05 1.6532595e-04] \n",
            "Epoch: 446 | Loss: [5.0011207e-04 2.3029337e-05 1.6294346e-04] \n",
            "Epoch: 447 | Loss: [4.9292948e-04 2.2701002e-05 1.6060243e-04] \n",
            "Epoch: 448 | Loss: [4.8585134e-04 2.2375025e-05 1.5829034e-04] \n",
            "Epoch: 449 | Loss: [4.7886616e-04 2.2051405e-05 1.5601882e-04] \n",
            "Epoch: 450 | Loss: [4.7198334e-04 2.1734588e-05 1.5377556e-04] \n",
            "Epoch: 451 | Loss: [4.6520177e-04 2.1424477e-05 1.5156028e-04] \n",
            "Epoch: 452 | Loss: [4.5851013e-04 2.1116595e-05 1.4939603e-04] \n",
            "Epoch: 453 | Loss: [4.5192777e-04 2.0815292e-05 1.4723578e-04] \n",
            "Epoch: 454 | Loss: [4.4543325e-04 2.0516154e-05 1.4512573e-04] \n",
            "Epoch: 455 | Loss: [4.390257e-04 2.021918e-05 1.430309e-04] \n",
            "Epoch: 456 | Loss: [4.3271415e-04 1.9928630e-05 1.4098527e-04] \n",
            "Epoch: 457 | Loss: [4.2649757e-04 1.9640182e-05 1.3895439e-04] \n",
            "Epoch: 458 | Loss: [4.2036505e-04 1.9358033e-05 1.3697171e-04] \n",
            "Epoch: 459 | Loss: [4.1432548e-04 1.9077925e-05 1.3500328e-04] \n",
            "Epoch: 460 | Loss: [4.0836813e-04 1.8803992e-05 1.3306009e-04] \n",
            "Epoch: 461 | Loss: [4.0250176e-04 1.8536148e-05 1.3115285e-04] \n",
            "Epoch: 462 | Loss: [3.9671583e-04 1.8270224e-05 1.2925935e-04] \n",
            "Epoch: 463 | Loss: [3.9101893e-04 1.8006222e-05 1.2740116e-04] \n",
            "Epoch: 464 | Loss: [3.8540066e-04 1.7748158e-05 1.2556711e-04] \n",
            "Epoch: 465 | Loss: [3.7986023e-04 1.7495946e-05 1.2375697e-04] \n",
            "Epoch: 466 | Loss: [3.74396826e-04 1.72415785e-05 1.21991565e-04] \n",
            "Epoch: 467 | Loss: [3.6901879e-04 1.6996935e-05 1.2022838e-04] \n",
            "Epoch: 468 | Loss: [3.6372512e-04 1.6754042e-05 1.1849880e-04] \n",
            "Epoch: 469 | Loss: [3.58487770e-04 1.65090205e-05 1.16792056e-04] \n",
            "Epoch: 470 | Loss: [3.5333319e-04 1.6273498e-05 1.1511816e-04] \n",
            "Epoch: 471 | Loss: [3.4826045e-04 1.6039667e-05 1.1346649e-04] \n",
            "Epoch: 472 | Loss: [3.4325087e-04 1.5807527e-05 1.1183685e-04] \n",
            "Epoch: 473 | Loss: [3.3832146e-04 1.5580845e-05 1.1022901e-04] \n",
            "Epoch: 474 | Loss: [3.3345379e-04 1.5355799e-05 1.0864275e-04] \n",
            "Epoch: 475 | Loss: [3.28664639e-04 1.51361019e-05 1.07087726e-04] \n",
            "Epoch: 476 | Loss: [3.2394446e-04 1.4917987e-05 1.0554391e-04] \n",
            "Epoch: 477 | Loss: [3.19283979e-04 1.47051114e-05 1.04021026e-04] \n",
            "Epoch: 478 | Loss: [3.1469954e-04 1.4493766e-05 1.0253818e-04] \n",
            "Epoch: 479 | Loss: [3.1017346e-04 1.4283950e-05 1.0106598e-04] \n",
            "Epoch: 480 | Loss: [3.0571350e-04 1.4079242e-05 9.9613942e-05] \n",
            "Epoch: 481 | Loss: [3.01327236e-04 1.38760115e-05 9.81724152e-05] \n",
            "Epoch: 482 | Loss: [2.9699731e-04 1.3677785e-05 9.6760152e-05] \n",
            "Epoch: 483 | Loss: [2.9273136e-04 1.3480985e-05 9.5376745e-05] \n",
            "Epoch: 484 | Loss: [2.8852056e-04 1.3289087e-05 9.4003299e-05] \n",
            "Epoch: 485 | Loss: [2.8437245e-04 1.3098565e-05 9.2639821e-05] \n",
            "Epoch: 486 | Loss: [2.8028627e-04 1.2909419e-05 9.1322749e-05] \n",
            "Epoch: 487 | Loss: [2.7625347e-04 1.2721648e-05 9.0015106e-05] \n",
            "Epoch: 488 | Loss: [2.722892e-04 1.253863e-05 8.871689e-05] \n",
            "Epoch: 489 | Loss: [2.68369244e-04 1.23602895e-05 8.74370235e-05] \n",
            "Epoch: 490 | Loss: [2.64516450e-04 1.21798985e-05 8.61930166e-05] \n",
            "Epoch: 491 | Loss: [2.6071462e-04 1.2007442e-05 8.4949133e-05] \n",
            "Epoch: 492 | Loss: [2.5696325e-04 1.1832934e-05 8.3723018e-05] \n",
            "Epoch: 493 | Loss: [2.5327699e-04 1.1662961e-05 8.2523140e-05] \n",
            "Epoch: 494 | Loss: [2.4963243e-04 1.1497450e-05 8.1331920e-05] \n",
            "Epoch: 495 | Loss: [2.4604422e-04 1.1329912e-05 8.0166443e-05] \n",
            "Epoch: 496 | Loss: [2.42511669e-04 1.11667905e-05 7.90178456e-05] \n",
            "Epoch: 497 | Loss: [2.39026791e-04 1.10080155e-05 7.78775429e-05] \n",
            "Epoch: 498 | Loss: [2.3558908e-04 1.0850377e-05 7.6762233e-05] \n",
            "Epoch: 499 | Loss: [2.3220533e-04 1.0693876e-05 7.5654971e-05] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-8dZqNX1hte",
        "outputId": "d9d57f81-04eb-4167-9637-dd72d2ff5ba5"
      },
      "source": [
        "hour_var = tf.constant([[4.0]])\n",
        "\n",
        "#예측값은 비슷하게 나옵니다.\n",
        "print(\"Prediction (after training)\", model(hour_var))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction (after training) tf.Tensor([[7.979483]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XZ8aj2Iwgrm"
      },
      "source": [
        ""
      ]
    }
  ]
}